{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Translate all Reddit and Twitter messages\n",
        "\n",
        "This parallelized code was run on an A100 GPU to translate **890K Reddit posts** and **1.2M Tweets**. Because it relies on the free model *Helsinki-NLP/opus-mt-mul-en*, which limits the input to **512 tokens**, each message must be **chunked into smaller segments** before translation. This significantly slows down the overall process, even when using a GPU.\n",
        "\n",
        "As a result, the translation took approximately **3 hours** for the 890K Reddit messages and around **18 hours** for the 1.2M Tweets.\n"
      ],
      "metadata": {
        "id": "mVE2m_2NXnHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP =====\n",
        "!pip install polars transformers accelerate sentencepiece -q\n",
        "\n",
        "import torch\n",
        "import polars as pl\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ===== MODEL & PIPELINE =====\n",
        "\n",
        "TRANSLATOR_MODEL = \"Helsinki-NLP/opus-mt-mul-en\"\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 0 if use_cuda else -1\n",
        "dtype = torch.float16 if use_cuda else torch.float32\n",
        "\n",
        "print(\"CUDA available:\", use_cuda)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TRANSLATOR_MODEL)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    TRANSLATOR_MODEL,\n",
        "    torch_dtype=dtype if use_cuda else torch.float32,\n",
        ")\n",
        "\n",
        "translation_pipe = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "MAX_MODEL_LEN = 512\n",
        "SAFE_INPUT_TOKENS = 384  # input truncation limit\n",
        "\n",
        "\n",
        "# ===== DATA LOADING =====\n",
        "\n",
        "def load_timestamp_text_2018_2019(csv_path: str, part: int | None = None) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the timestamp/text CSV using Polars, ensure chronological order,\n",
        "    and optionally return one of six equal-sized time-slices for 2018.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : str\n",
        "        Path to the CSV file.\n",
        "    part : int or None\n",
        "        If 1-6, return the corresponding 1/6 time-slice of [2018-01-01, 2019-01-01).\n",
        "        If None, return the full dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pl.DataFrame\n",
        "    \"\"\"\n",
        "    schema = {\n",
        "        \"timestamp\": pl.Datetime,\n",
        "        \"text\": pl.Utf8,\n",
        "    }\n",
        "\n",
        "    df = (\n",
        "        pl.read_csv(\n",
        "            csv_path,\n",
        "            schema=schema,\n",
        "            has_header=True,\n",
        "            separator=\",\",\n",
        "        )\n",
        "        .sort(\"timestamp\")  # ensure time order\n",
        "    )\n",
        "\n",
        "    # If no part requested, return full df\n",
        "    if part is None:\n",
        "        return df\n",
        "\n",
        "    # Validate part\n",
        "    if not (1 <= part <= 6):\n",
        "        raise ValueError(\"part must be an integer between 1 and 6, or None.\")\n",
        "\n",
        "    # Fixed absolute boundaries for the year\n",
        "    year_start = datetime(2018, 1, 1)\n",
        "    year_end = datetime(2019, 1, 1)\n",
        "\n",
        "    # Split [year_start, year_end) into 6 equal time intervals\n",
        "    total_span = year_end - year_start\n",
        "    step = total_span / 6  # timedelta\n",
        "\n",
        "    slice_start = year_start + step * (part - 1)\n",
        "    slice_end = year_start + step * part\n",
        "\n",
        "    # Filter rows whose timestamp falls into [slice_start, slice_end)\n",
        "    df_part = df.filter(\n",
        "        (pl.col(\"timestamp\") >= slice_start) &\n",
        "        (pl.col(\"timestamp\") < slice_end)\n",
        "    )\n",
        "\n",
        "    return df_part\n",
        "\n",
        "\n",
        "# ===== TRANSLATION HELPERS =====\n",
        "\n",
        "def trim_for_model(text: str) -> str:\n",
        "    \"\"\"Tokenize and hard-trim long inputs so we stay under SAFE_INPUT_TOKENS.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "    ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    if len(ids) > SAFE_INPUT_TOKENS:\n",
        "        ids = ids[:SAFE_INPUT_TOKENS]\n",
        "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def translate_list(texts, batch_size: int = 128):\n",
        "    \"\"\"\n",
        "    Translate a list of texts (may contain empty / non-string entries).\n",
        "    Returns a list of English strings of the same length.\n",
        "    \"\"\"\n",
        "    n = len(texts)\n",
        "    translations = [\"\"] * n\n",
        "\n",
        "    # Indices of non-empty strings\n",
        "    idxs = [i for i, t in enumerate(texts) if isinstance(t, str) and t.strip()]\n",
        "    if not idxs:\n",
        "        return translations\n",
        "\n",
        "    to_translate = [trim_for_model(texts[i]) for i in idxs]\n",
        "\n",
        "    # Run pipeline on the non-empty subset\n",
        "    outputs = translation_pipe(\n",
        "        to_translate,\n",
        "        batch_size=batch_size,\n",
        "        truncation=True,\n",
        "        max_length=MAX_MODEL_LEN,  # generation cap (output)\n",
        "    )\n",
        "\n",
        "    # HF translation pipeline returns list of dicts: [{\"translation_text\": \"...\"}]\n",
        "    for i, out in zip(idxs, outputs):\n",
        "        translations[i] = out.get(\"translation_text\", \"\")\n",
        "\n",
        "    return translations\n",
        "\n",
        "\n",
        "def translate_df(\n",
        "    df: pl.DataFrame,\n",
        "    text_col: str = \"text\",\n",
        "    translation_col: str = \"text_en\",\n",
        "    batch_size: int = 128,\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"Translate `df[text_col]` into English and add a new column `translation_col`.\"\"\"\n",
        "    texts = df[text_col].to_list()\n",
        "    translations = translate_list(texts, batch_size=batch_size)\n",
        "    return df.with_columns(pl.Series(translation_col, translations))\n",
        "\n",
        "\n",
        "def translate_func(input_path, output_path, part):\n",
        "  # Load only part=1 of 2018–2019 tweets\n",
        "    df = load_timestamp_text_2018_2019(input_path, part=part)\n",
        "    print(\"Loaded shape:\", df.shape)\n",
        "\n",
        "    # Choose batch size based on device\n",
        "    batch_size = 256\n",
        "    print(\"Using batch size:\", batch_size)\n",
        "\n",
        "    # Chunking to be safe with memory and to keep things responsive\n",
        "    chunk_size = 100000  # about 10 chunks for 207,198 rows\n",
        "    translated_chunks = []\n",
        "\n",
        "    for start in range(0, len(df), chunk_size):\n",
        "        end = min(start + chunk_size, len(df))\n",
        "        print(f\"Translating rows {start}–{end}...\")\n",
        "\n",
        "        df_chunk = df.slice(start, end - start)\n",
        "        df_chunk_translated = translate_df(\n",
        "            df_chunk,\n",
        "            text_col=\"text\",\n",
        "            translation_col=\"text_en\",\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        translated_chunks.append(df_chunk_translated)\n",
        "\n",
        "    # Concatenate all translated chunks\n",
        "    df_all = pl.concat(translated_chunks)\n",
        "    print(\"Final shape:\", df_all.shape)\n",
        "    print(df_all.head(5))\n",
        "\n",
        "    # Save to CSV\n",
        "    df_all.write_csv(output_path)\n",
        "    print(\"Saved to:\", output_path)\n",
        "\n",
        "# ===== MAIN: LOAD, TRANSLATE IN CHUNKS, SAVE =====\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\"\n",
        "output_path = \"/content/drive/MyDrive/SDA/tweets_test_1.csv\"\n",
        "\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_1.csv\", 1)\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_2.csv\", 2)\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_3.csv\", 3)\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_4.csv\", 4)\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_5.csv\", 5)\n",
        "translate_func(\"/content/drive/MyDrive/SDA/tweets_2018_2019.csv\", \"/content/drive/MyDrive/SDA/tweets_test_6.csv\", 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXCe-PuBBIYE",
        "outputId": "d7888c3a-2add-4cb5-da41-6a259de5d957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CUDA available: True\n",
            "Using device: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded shape: (207198, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–100000...\n",
            "Translating rows 100000–200000...\n",
            "Translating rows 200000–207198...\n",
            "Final shape: (207198, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-01-01 00:00:01 ┆ Bitcoin - BTC                   ┆ Bitcoin - BTC Price: $13,941.1… │\n",
            "│                     ┆ Price: $13,941.1…               ┆                                 │\n",
            "│ 2018-01-01 00:00:01 ┆ 2018年01月01日 10:00            ┆ 01 January 2018 10:00 [DOGE] 1… │\n",
            "│                     ┆ [DOGE建]                        ┆                                 │\n",
            "│                     ┆ 1XP=…                           ┆                                 │\n",
            "│ 2018-01-01 00:00:02 ┆ 01/01 10:00現在                 ┆ 01/01 10:00 #Bitcoin: 1,662,70… │\n",
            "│                     ┆                                 ┆                                 │\n",
            "│                     ┆ #Bitcoin : 1,66…                ┆                                 │\n",
            "│ 2018-01-01 00:00:03 ┆ Cotizaciones al 31/12/2017 10:… ┆ Contributions to 31/12/2017 10… │\n",
            "│ 2018-01-01 00:00:03 ┆ Cotización del Bitcoin Cash: 2… ┆ Bitcoin Cash Rating: 2,046 10.… │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_1.csv\n",
            "Loaded shape: (314636, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–100000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating rows 100000–200000...\n",
            "Translating rows 200000–300000...\n",
            "Translating rows 300000–314636...\n",
            "Final shape: (314636, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-03-02 20:00:00 ┆ Today's 4pm ET auction: 246.00… ┆ Today's 4pm ET auction: 246.00… │\n",
            "│ 2018-03-02 20:00:01 ┆ Bitcoin - BTC                   ┆ Bitcoin - BTC Price: $11,080.9… │\n",
            "│                     ┆ Price: $11,080.9…               ┆                                 │\n",
            "│ 2018-03-02 20:00:02 ┆ 03/03 06:00現在(Zaif調べ)       ┆ 03/03 06:00 now (Zaif survey) … │\n",
            "│                     ┆                                 ┆                                 │\n",
            "│                     ┆ #Bitcoi…                        ┆                                 │\n",
            "│ 2018-03-02 20:00:02 ┆ Cotización del Bitcoin Cash: 1… ┆ Bitcoin Cash Rating: 1,043 80.… │\n",
            "│ 2018-03-02 20:00:02 ┆ 2018/03/03 06:00                ┆ 2018/03/03 06:00 #Binance Ghan… │\n",
            "│                     ┆ #Binance 格安コイ…              ┆                                 │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_2.csv\n",
            "Loaded shape: (254821, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–100000...\n",
            "Translating rows 100000–200000...\n",
            "Translating rows 200000–254821...\n",
            "Final shape: (254821, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-05-02 16:00:00 ┆ 02May2018 18:00 UTC #Bitcoin l… ┆ 02May2018 18:00 UTC #Bitcoin l… │\n",
            "│ 2018-05-02 16:00:01 ┆ 1 BTC = 32260.01000000 BRL em … ┆ 1 BTC = 32260.01000000 BRL on … │\n",
            "│ 2018-05-02 16:00:01 ┆ 02May2018 18:00 UTC #Bitcoin #… ┆ 02May2018 18:00 UTC #Bitcoin #… │\n",
            "│ 2018-05-02 16:00:02 ┆  02/05/2018 - 21:00             ┆ ==============================… │\n",
            "│                     ┆ ==========…                     ┆                                 │\n",
            "│ 2018-05-02 16:00:02 ┆ 2018/05/03 03:00                ┆ 2018/05/03 03:00 #Binance Ghan… │\n",
            "│                     ┆ #Binance 格安コイ…              ┆                                 │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_3.csv\n",
            "Loaded shape: (253018, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–100000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating rows 100000–200000...\n",
            "Translating rows 200000–253018...\n",
            "Final shape: (253018, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-07-02 12:00:00 ┆ Bitcoin (4.02): $6,636.81       ┆ Bitcoin (4.02): $6,636.81 Ethe… │\n",
            "│                     ┆ Ethe…                           ┆                                 │\n",
            "│ 2018-07-02 12:00:01 ┆ 2018/07/02（月）23:00           ┆ 2018/07/02 (month) 23:00 Bitco… │\n",
            "│                     ┆ ビットコインの価格は7…          ┆                                 │\n",
            "│ 2018-07-02 12:00:01 ┆ Bitcoin - BTC                   ┆ Bitcoin - BTC Price: $6,642.98… │\n",
            "│                     ┆ Price: $6,642.98…               ┆                                 │\n",
            "│ 2018-07-02 12:00:02 ┆ Cotización del Bitcoin Cash: 6… ┆ Bitcoin Cash Rating: 673 30.€ … │\n",
            "│ 2018-07-02 12:00:02 ┆ #cryptocurrency Price Analysis… ┆ #cryptocurrency Price Analysis… │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_4.csv\n",
            "Loaded shape: (65364, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–65364...\n",
            "Final shape: (65364, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-09-01 08:00:00 ┆ Bitcoin (0.09): $7,070.23       ┆ Bitcoin (0.09): $7.070.23 Ethe… │\n",
            "│                     ┆ Ethe…                           ┆                                 │\n",
            "│ 2018-09-01 08:00:01 ┆ 1 BTC = 29195.67994000 BRL em … ┆ 1 BTC = 29195.67994000 BRL on … │\n",
            "│ 2018-09-01 08:00:01 ┆ 09/01 19:00現在                 ┆ 09/01 19:00 now #Bitcoin: 783,… │\n",
            "│                     ┆                                 ┆                                 │\n",
            "│                     ┆ #Bitcoin : 783,…                ┆                                 │\n",
            "│ 2018-09-01 08:00:02 ┆ #cryptocurrency Price Analysis… ┆ #cryptocurrency Price Analysis… │\n",
            "│ 2018-09-01 08:00:02 ┆ 2018/09/01 19:00                ┆ 2018/09/01 19:00 #Binance Gink… │\n",
            "│                     ┆ #Binance 格安コイ…              ┆                                 │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_5.csv\n",
            "Loaded shape: (123949, 2)\n",
            "Using batch size: 256\n",
            "Translating rows 0–100000...\n",
            "Translating rows 100000–123949...\n",
            "Final shape: (123949, 3)\n",
            "shape: (5, 3)\n",
            "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
            "│ timestamp           ┆ text                            ┆ text_en                         │\n",
            "│ ---                 ┆ ---                             ┆ ---                             │\n",
            "│ datetime[μs]        ┆ str                             ┆ str                             │\n",
            "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
            "│ 2018-11-01 04:00:00 ┆ 最も高くBTC/JPYを売れるのは？(2 ┆ (2018-11-01 14:00:02, now) bit… │\n",
            "│                     ┆ 018-11-01 …                     ┆                                 │\n",
            "│ 2018-11-01 04:00:00 ┆ 最も安くBTC/JPYを買えるのは？(2 ┆ Zaif 697075.0 bitFlyer 709490.… │\n",
            "│                     ┆ 018-11-01 …                     ┆                                 │\n",
            "│ 2018-11-01 04:00:01 ┆ 最もBTC/JPYの取引量が多いのは？ ┆ (2018-11-01 14:00:02) Liquid 1… │\n",
            "│                     ┆ (2018-11-01…                    ┆                                 │\n",
            "│ 2018-11-01 04:00:01 ┆ Bitcoin - BTC                   ┆ Bitcoin - BTC Price: $6,502.14… │\n",
            "│                     ┆ Price: $6,502.14…               ┆                                 │\n",
            "│ 2018-11-01 04:00:01 ┆ 最もBTC/JPYのスプレッドが狭いの ┆ (2018-11-01 14:00:02) Liquid 1… │\n",
            "│                     ┆ は？(2018-11-…                  ┆                                 │\n",
            "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n",
            "Saved to: /content/drive/MyDrive/SDA/tweets_test_6.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis all Reddit and Twitter messages\n",
        "\n",
        "\n",
        "This script performs large-scale sentiment analysis on translated Reddit and Twitter datasets using the **CardiffNLP Twitter RoBERTa sentiment model**. The code is optimized for GPU execution (A100 in this case), leveraging **PyTorch**, **Transformers**, and **Polars** for efficient preprocessing and batching."
      ],
      "metadata": {
        "id": "ODRGLpZ9ZAQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP =====\n",
        "!pip install -q polars transformers accelerate sentencepiece\n",
        "\n",
        "import re\n",
        "import torch\n",
        "import polars as pl\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ===== DEVICE =====\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "dtype = torch.float16 if use_cuda else torch.float32\n",
        "print(\"CUDA available:\", use_cuda, \"| device:\", device)\n",
        "\n",
        "# ===== SENTIMENT MODEL (GPU) =====\n",
        "# Great for tweet-like text; works fine on translated reddit too.\n",
        "SENTIMENT_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "sent_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL)\n",
        "sent_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    SENTIMENT_MODEL,\n",
        "    torch_dtype=dtype if use_cuda else torch.float32,\n",
        ").to(device)\n",
        "sent_model.eval()\n",
        "\n",
        "# Robust mapping to NEG/NEU/POS indices\n",
        "id2label = {int(k): v for k, v in sent_model.config.id2label.items()}\n",
        "labels_lower = {v.lower(): k for k, v in id2label.items()}\n",
        "if {\"negative\", \"neutral\", \"positive\"}.issubset(labels_lower):\n",
        "    NEG_ID = labels_lower[\"negative\"]\n",
        "    NEU_ID = labels_lower[\"neutral\"]\n",
        "    POS_ID = labels_lower[\"positive\"]\n",
        "else:\n",
        "    # Common convention for this CardiffNLP model family\n",
        "    NEG_ID, NEU_ID, POS_ID = 0, 1, 2\n",
        "\n",
        "# Light cleanup (helps a bit, cheap)\n",
        "_URL = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "_MENTION = re.compile(r\"@\\w+\")\n",
        "_WS = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if not isinstance(t, str) or not t.strip():\n",
        "        return \"\"\n",
        "    t = _URL.sub(\" \", t)\n",
        "    t = _MENTION.sub(\" \", t)\n",
        "    return _WS.sub(\" \", t).strip()\n",
        "\n",
        "def sentiment_list_gpu(texts, batch_size: int = 1024, max_length: int = 256):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      scores: list[float] in [-1, 1]  (P(pos) - P(neg))\n",
        "      labels: list[str] in {\"pos\",\"neu\",\"neg\"}\n",
        "    \"\"\"\n",
        "    n = len(texts)\n",
        "    scores = [0.0] * n\n",
        "    labels = [\"neu\"] * n\n",
        "\n",
        "    idxs = [i for i, t in enumerate(texts) if isinstance(t, str) and t.strip()]\n",
        "    if not idxs:\n",
        "        return scores, labels\n",
        "\n",
        "    cleaned = [clean_text(texts[i]) for i in idxs]\n",
        "\n",
        "    for start in range(0, len(cleaned), batch_size):\n",
        "        batch_texts = cleaned[start:start + batch_size]\n",
        "\n",
        "        enc = sent_tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            logits = sent_model(**enc).logits\n",
        "            probs = torch.softmax(logits, dim=-1).float().cpu()\n",
        "\n",
        "        for j in range(probs.size(0)):\n",
        "            p = probs[j]\n",
        "            score = float(p[POS_ID] - p[NEG_ID])      # [-1, 1]\n",
        "            pred = int(torch.argmax(p).item())\n",
        "            lab = \"pos\" if pred == POS_ID else \"neg\" if pred == NEG_ID else \"neu\"\n",
        "\n",
        "            orig_i = idxs[start + j]\n",
        "            scores[orig_i] = score\n",
        "            labels[orig_i] = lab\n",
        "\n",
        "    return scores, labels\n",
        "\n",
        "def add_sentiment_df(\n",
        "    df: pl.DataFrame,\n",
        "    text_col: str = \"text_en\",\n",
        "    score_col: str = \"sentiment_score\",\n",
        "    label_col: str = \"sentiment_label\",\n",
        "    batch_size: int = 1024,\n",
        "    max_length: int = 256,\n",
        ") -> pl.DataFrame:\n",
        "    texts = df[text_col].to_list()\n",
        "    scores, labels = sentiment_list_gpu(texts, batch_size=batch_size, max_length=max_length)\n",
        "    return df.with_columns(\n",
        "        pl.Series(score_col, scores),\n",
        "        pl.Series(label_col, labels),\n",
        "    )\n",
        "\n",
        "# ===== CHUNKED FILE PROCESSOR =====\n",
        "def score_file_in_chunks(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    text_col: str = \"text_en\",\n",
        "    chunk_size: int = 100_000,\n",
        "    batch_size: int = 1024,\n",
        "    max_length: int = 256,\n",
        "):\n",
        "    df = pl.read_csv(input_path)  # keep simple; if huge, run per-part files like you already do\n",
        "    print(\"Loaded:\", input_path, \"shape:\", df.shape)\n",
        "\n",
        "    out_chunks = []\n",
        "    for start in range(0, len(df), chunk_size):\n",
        "        end = min(start + chunk_size, len(df))\n",
        "        print(f\"Scoring rows {start}–{end}...\")\n",
        "\n",
        "        chunk = df.slice(start, end - start)\n",
        "        chunk = add_sentiment_df(\n",
        "            chunk,\n",
        "            text_col=text_col,\n",
        "            batch_size=batch_size,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        out_chunks.append(chunk)\n",
        "\n",
        "    out = pl.concat(out_chunks)\n",
        "    out.write_csv(output_path)\n",
        "    print(\"Saved:\", output_path, \"shape:\", out.shape)\n",
        "\n",
        "# ===== EXAMPLE: score your 6 translated parts =====\n",
        "for i in range(1, 7):\n",
        "    inp = f\"/content/drive/MyDrive/SDA/tweets_{i}.csv\"          # contains text_en already\n",
        "    out = f\"/content/drive/MyDrive/SDA/tweets_{i}_sent.csv\"\n",
        "    score_file_in_chunks(inp, out, text_col=\"text_en\", chunk_size=100_000, batch_size=1024, max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGn_P7T_DrCk",
        "outputId": "cc2105d4-1f37-455b-d589-fb32569360b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CUDA available: True | device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: /content/drive/MyDrive/SDA/tweets_1.csv shape: (207198, 3)\n",
            "Scoring rows 0–100000...\n",
            "Scoring rows 100000–200000...\n",
            "Scoring rows 200000–207198...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_1_sent.csv shape: (207198, 5)\n",
            "Loaded: /content/drive/MyDrive/SDA/tweets_2.csv shape: (314636, 3)\n",
            "Scoring rows 0–100000...\n",
            "Scoring rows 100000–200000...\n",
            "Scoring rows 200000–300000...\n",
            "Scoring rows 300000–314636...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_2_sent.csv shape: (314636, 5)\n",
            "Loaded: /content/drive/MyDrive/SDA/tweets_3.csv shape: (254821, 3)\n",
            "Scoring rows 0–100000...\n",
            "Scoring rows 100000–200000...\n",
            "Scoring rows 200000–254821...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_3_sent.csv shape: (254821, 5)\n",
            "Loaded: /content/drive/MyDrive/SDA/tweets_4.csv shape: (253018, 3)\n",
            "Scoring rows 0–100000...\n",
            "Scoring rows 100000–200000...\n",
            "Scoring rows 200000–253018...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_4_sent.csv shape: (253018, 5)\n",
            "Loaded: /content/drive/MyDrive/SDA/tweets_5.csv shape: (65364, 3)\n",
            "Scoring rows 0–65364...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_5_sent.csv shape: (65364, 5)\n",
            "Loaded: /content/drive/MyDrive/SDA/tweets_6.csv shape: (123949, 3)\n",
            "Scoring rows 0–100000...\n",
            "Scoring rows 100000–123949...\n",
            "Saved: /content/drive/MyDrive/SDA/tweets_6_sent.csv shape: (123949, 5)\n"
          ]
        }
      ]
    }
  ]
}